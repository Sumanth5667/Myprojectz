{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ce19b0",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b89b",
   "metadata": {},
   "source": [
    "Q1. Min-Max Scaling is a data preprocessing technique used to transform features in a dataset to a specific range, usually between 0 and 1. It's done by subtracting the minimum value of the feature from each data point and then dividing by the range (the difference between the maximum and minimum values). The formula for Min-Max scaling is:\n",
    "\n",
    "Scaled Value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "For example, if you have a dataset of house sizes (in square feet) and you want to scale it using Min-Max scaling, where the smallest house is 800 sq. ft. and the largest is 2000 sq. ft., you can scale a house of 1000 sq. ft. as follows:\n",
    "\n",
    "Scaled Value = (1000 - 800) / (2000 - 800) = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3de855",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b5f1a",
   "metadata": {},
   "source": [
    "Q2. Unit Vector Scaling is a technique where each data point is scaled to have a unit norm, meaning it's rescaled to have a length or magnitude of 1. This ensures that all the data points lie on the unit hypersphere. Unlike Min-Max scaling, it doesn't necessarily map values to a specific range but ensures that the magnitude or length of the data vector is 1.\n",
    "\n",
    "For example, if you have a dataset of 2D points (x, y) and you want to scale them to unit vectors, you'd calculate the magnitude (length) of each vector and divide the x and y components by that magnitude to make it 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb6229",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide anexample to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d90fdd",
   "metadata": {},
   "source": [
    "Q3. Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a dataset into a new coordinate system where the data's variance is maximized along the principal components (new axes). It's used to reduce the dimensionality of a dataset while retaining as much of the original variance as possible.\n",
    "\n",
    "For example, if you have a dataset with many correlated features like height, weight, and age, PCA can help create new uncorrelated features (principal components) that capture the most important information in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615865e",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e103ef",
   "metadata": {},
   "source": [
    "Q4. PCA is a technique used for both dimensionality reduction and feature extraction. In dimensionality reduction, PCA reduces the number of features while retaining most of the variance. In feature extraction, PCA constructs new features (principal components) that are linear combinations of the original features. These new features are designed to capture the most information from the data.\n",
    "\n",
    "For example, in a dataset containing height, weight, and age, PCA can create new features like \"body mass index\" by combining the original features. This can be valuable in cases where these newly created features are more informative or less correlated than the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbebf77",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccb5d4",
   "metadata": {},
   "source": [
    "Q5. To use Min-Max scaling in a food delivery recommendation system project with features like price, rating, and delivery time, you would follow these steps:\n",
    "\n",
    "Identify the minimum and maximum values for each feature (e.g., minimum and maximum prices, ratings, and delivery times).\n",
    "Use the Min-Max scaling formula to scale each feature to the range [0, 1].\n",
    "The scaled values are now ready for use in your recommendation system, and they are on the same scale, so no feature dominates due to its scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e22ccf",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea8ef6",
   "metadata": {},
   "source": [
    "Q6. In a stock price prediction project with many features, you can use PCA for dimensionality reduction as follows:\n",
    "\n",
    "Standardize your dataset (mean=0, variance=1) to ensure all features are on the same scale.\n",
    "Apply PCA to reduce the dimensionality while retaining a certain amount of explained variance (e.g., retaining 95% of variance).\n",
    "The reduced dataset will have fewer features (principal components) that capture the most important information.\n",
    "You can then use these reduced features to build your stock price prediction model, which will be computationally more efficient and may reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5f083a",
   "metadata": {},
   "source": [
    "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06afb3eb",
   "metadata": {},
   "source": [
    "Q7. For Min-Max scaling to transform the dataset [1, 5, 10, 15, 20] to a range of -1 to 1:\n",
    "\n",
    "Find the minimum value (X_min) and maximum value (X_max) in the dataset: X_min = 1, X_max = 20.\n",
    "\n",
    "Apply the Min-Max scaling formula for each value in the dataset:\n",
    "\n",
    "Scaled Value = (X - X_min) / (X_max - X_min)\n",
    "For each value:\n",
    "\n",
    "For X = 1: Scaled Value = (1 - 1) / (20 - 1) = 0\n",
    "For X = 5: Scaled Value = (5 - 1) / (20 - 1) = 0.25\n",
    "For X = 10: Scaled Value = (10 - 1) / (20 - 1) = 0.45\n",
    "For X = 15: Scaled Value = (15 - 1) / (20 - 1) = 0.65\n",
    "For X = 20: Scaled Value = (20 - 1) / (20 - 1) = 1\n",
    "So, the scaled dataset in the range -1 to 1 would be [-1, -0.5, 0, 0.5, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acbea14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Find the minimum and maximum values\n",
    "min_value = min(data)\n",
    "max_value = max(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = -1 + 2 * (data - min_value) / (max_value - min_value)\n",
    "\n",
    "# Print the scaled data\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c21074",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef2f531",
   "metadata": {},
   "source": [
    "Q8. To perform Feature Extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], you would typically follow these steps:\n",
    "\n",
    "Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "Apply PCA to the standardized data.\n",
    "Calculate the explained variance for each principal component.\n",
    "Decide how many principal components to retain based on the explained variance. You might choose a threshold, like retaining components that explain 95% of the variance.\n",
    "The retained principal components can be used as the reduced feature set in your model.\n",
    "The number of principal components to retain should be based on the amount of variance they explain and the trade-off between dimensionality reduction and information loss. Typically, you would retain as many components as needed to capture a significant portion of the variance while reducing dimensionality. The decision on how many components to keep is often based on the specific project's requirements and constraints.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4899e303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained Principal Components:\n",
      "[[ 0.04630879  1.64305501]\n",
      " [-2.58901214 -0.7822141 ]\n",
      " [ 2.91655833 -0.64801459]\n",
      " [-1.81942304 -0.02951849]\n",
      " [ 1.44556806 -0.18330783]]\n",
      "Explained Variance Ratio:\n",
      "[0.82445632 0.15063539]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a sample dataset (replace this with your actual data)\n",
    "data = np.array([\n",
    "    [175, 70, 35, 0, 120],\n",
    "    [162, 55, 28, 1, 130],\n",
    "    [185, 85, 42, 0, 140],\n",
    "    [168, 62, 31, 1, 125],\n",
    "    [177, 75, 39, 0, 135]\n",
    "])\n",
    "\n",
    "# Standardize the data (mean=0, variance=1)\n",
    "mean = np.mean(data, axis=0)\n",
    "std_dev = np.std(data, axis=0)\n",
    "standardized_data = (data - mean) / std_dev\n",
    "\n",
    "# Create a PCA instance and specify the number of components to retain\n",
    "num_components = 2  # Adjust as needed\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Fit and transform the data to extract principal components\n",
    "principal_components = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Print the retained principal components\n",
    "print(\"Retained Principal Components:\")\n",
    "print(principal_components)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a5226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
