{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32d42d7",
   "metadata": {},
   "source": [
    "### Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4b3411",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare the means of two or more groups to determine if there are statistically significant differences between them. ANOVA makes certain assumptions about the data, and violations of these assumptions can impact the validity of the results. The key assumptions of ANOVA include:\n",
    "\n",
    "1. **Independence of Observations:** The observations within each group must be independent of each other. This means that the value of one observation does not affect the value of another observation.\n",
    "\n",
    "2. **Normality:** The residuals (the differences between the observed values and the group means) should follow a normal distribution within each group. This assumption is more important for smaller sample sizes, as ANOVA is robust to departures from normality for larger sample sizes.\n",
    "\n",
    "3. **Homogeneity of Variances (Homoscedasticity):** The variances of the residuals should be approximately equal across all groups. In other words, the spread of the residuals should be consistent across groups.\n",
    "\n",
    "Examples of Violations and Their Impact:\n",
    "\n",
    "1. **Independence of Observations:**\n",
    "   - Violation: Observations within a group are correlated, such as repeated measurements taken over time on the same subjects.\n",
    "   - Impact: Correlated observations can lead to pseudoreplication, inflating the apparent degrees of freedom and leading to incorrect conclusions.\n",
    "\n",
    "2. **Normality:**\n",
    "   - Violation: The residuals within each group do not follow a normal distribution, especially for small sample sizes.\n",
    "   - Impact: If the normality assumption is violated, the p-values and confidence intervals produced by ANOVA may be inaccurate. Non-parametric alternatives like the Kruskal-Wallis test can be used in such cases.\n",
    "\n",
    "3. **Homogeneity of Variances:**\n",
    "   - Violation: The variances of the residuals differ significantly between groups, indicating heteroscedasticity.\n",
    "   - Impact: Heteroscedasticity can lead to unequal variability in the treatment effects across groups, affecting the validity of the F-test and the significance of differences between group means.\n",
    "\n",
    "Addressing Violations:\n",
    "- For violations of normality and homoscedasticity, transformation of the data (e.g., logarithmic or square root transformations) may help to meet the assumptions.\n",
    "- If transformations do not work, non-parametric tests like the Kruskal-Wallis test can be used instead of ANOVA.\n",
    "- If independence is violated due to correlated observations, repeated measures ANOVA or mixed-effects models might be more appropriate.\n",
    "\n",
    "It's important to check the assumptions before conducting ANOVA and consider alternative methods if the assumptions are not met to ensure valid and reliable results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a17f130",
   "metadata": {},
   "source": [
    "### Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa08437",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) comes in several forms, each designed for specific situations and types of data. The three main types of ANOVA are:\n",
    "\n",
    "1. **One-Way ANOVA:**\n",
    "   - Situation: Used when comparing means of a continuous dependent variable across three or more independent (categorical) groups.\n",
    "   - Example: Comparing the average scores of students from three different schools based on their school type (public, private, charter).\n",
    "\n",
    "2. **Two-Way ANOVA:**\n",
    "   - Situation: Used when examining the effects of two independent categorical variables on a continuous dependent variable. It allows for the investigation of interaction effects between the two factors.\n",
    "   - Example: Analyzing the effects of both gender and different teaching methods on student test scores.\n",
    "\n",
    "3. **Repeated Measures ANOVA (or within-subjects ANOVA):**\n",
    "   - Situation: Used when analyzing the effects of a single independent variable (repeated measurements or conditions) on a continuous dependent variable collected from the same subjects.\n",
    "   - Example: Measuring the effects of different time points (before, during, after treatment) on patients' blood pressure levels.\n",
    "\n",
    "Each type of ANOVA is appropriate for a specific research design and data structure. It's important to select the appropriate type of ANOVA based on the study's objectives and the nature of the data being analyzed. Additionally, ANOVA assumes certain assumptions, such as normality, homogeneity of variances, and independence of observations, which should be checked and addressed if violated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc457961",
   "metadata": {},
   "source": [
    "### Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4bf22",
   "metadata": {},
   "source": [
    "The partitioning of variance in Analysis of Variance (ANOVA) refers to the process of decomposing the total variability observed in the data into different sources of variation. ANOVA achieves this by dividing the total sum of squares (SST) into meaningful components that provide insights into the contributions of different factors or variables to the observed variability. The partitioning of variance allows us to assess the significance of these components and draw conclusions about the relationships between variables.\n",
    "\n",
    "In ANOVA, the partitioning of variance involves calculating three main sums of squares:\n",
    "\n",
    "1. **Between-Groups Sum of Squares (SSB):** This represents the variability between the group means. It measures how much the group means differ from the overall mean of the entire dataset.\n",
    "\n",
    "2. **Within-Groups Sum of Squares (SSW or SSE):** This represents the variability within each group. It measures the variation of individual data points within each group and provides an estimate of the random variability.\n",
    "\n",
    "3. **Total Sum of Squares (SST):** This is the total variability in the dataset and is the sum of the between-groups and within-groups sums of squares: SST = SSB + SSW.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. **Identifying Sources of Variation:** By decomposing the total variability, ANOVA helps us identify and quantify the contributions of different factors or variables to the overall variance. This is crucial for understanding which variables are responsible for significant differences between groups.\n",
    "\n",
    "2. **Assessing Significance:** The partitioning of variance allows us to calculate the F-statistic, which is used to assess the significance of the differences between group means. This helps us determine whether the observed differences are likely due to genuine effects or are simply due to random variability.\n",
    "\n",
    "3. **Interpreting Results:** Understanding how the total variance is distributed between different sources of variation provides valuable insights into the relationships between variables. It allows researchers to make informed interpretations about the impact of different factors on the outcome variable.\n",
    "\n",
    "4. **Model Comparison:** In situations where multiple models are being compared (e.g., different levels of complexity), the partitioning of variance helps quantify how well each model explains the variability in the data.\n",
    "\n",
    "Overall, the partitioning of variance in ANOVA is a fundamental concept that enables researchers to assess the significance of group differences and make informed conclusions about the relationships between variables based on the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa30d6a",
   "metadata": {},
   "source": [
    "### Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "### sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209c4948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 280.0\n",
      "Explained Sum of Squares (SSE): 250.0\n",
      "Residual Sum of Squares (SSR): 30.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data for each group\n",
    "group_data = [\n",
    "    np.array([5, 6, 7, 8, 9]),\n",
    "    np.array([10, 11, 12, 13, 14]),\n",
    "    np.array([15, 16, 17, 18, 19])\n",
    "]\n",
    "\n",
    "# Calculate total mean\n",
    "total_data = np.concatenate(group_data)\n",
    "total_mean = np.mean(total_data)\n",
    "\n",
    "# Calculate SST\n",
    "sst = np.sum((total_data - total_mean)**2)\n",
    "\n",
    "# Calculate SSE and SSR\n",
    "sse = 0\n",
    "for data in group_data:\n",
    "    group_mean = np.mean(data)\n",
    "    sse += len(data) * (group_mean - total_mean)**2\n",
    "\n",
    "ssr = sst - sse\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd535d",
   "metadata": {},
   "source": [
    "### Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76c7230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect A: 28.800000000000097\n",
      "Main Effect B: 9.799999999999944\n",
      "Interaction Effect: 5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'FactorA': np.repeat(['A1', 'A2'], [10, 10]),\n",
    "    'FactorB': np.tile(['B1', 'B2'], 10),\n",
    "    'Response': np.array([\n",
    "        23, 25, 27, 30, 28, 20, 22, 25, 26, 24,\n",
    "        32, 34, 30, 36, 38, 18, 19, 22, 24, 21\n",
    "    ])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = ols('Response ~ FactorA + FactorB + FactorA:FactorB', data=df).fit()\n",
    "\n",
    "# Get the ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Calculate main effects\n",
    "main_effect_A = anova_table.loc['FactorA', 'mean_sq']\n",
    "main_effect_B = anova_table.loc['FactorB', 'mean_sq']\n",
    "\n",
    "# Calculate interaction effect\n",
    "interaction_effect = anova_table.loc['FactorA:FactorB', 'mean_sq']\n",
    "\n",
    "print(\"Main Effect A:\", main_effect_A)\n",
    "print(\"Main Effect B:\", main_effect_B)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf47c7",
   "metadata": {},
   "source": [
    "### Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd1a3f",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences between the means of three or more independent groups. The p-value associated with the F-statistic indicates the probability of obtaining the observed results (or more extreme results) if the null hypothesis is true. The null hypothesis for a one-way ANOVA states that there are no significant differences between the group means.\n",
    "\n",
    "In your scenario, you obtained an F-statistic of 5.23 and a p-value of 0.02. Let's interpret these results:\n",
    "\n",
    "1. **F-Statistic (5.23):**\n",
    "   - The F-statistic is a measure of the ratio of the variance between groups (explained variance) to the variance within groups (unexplained variance).\n",
    "   - A larger F-statistic suggests that the variability between group means is larger relative to the variability within groups.\n",
    "   - In your case, an F-statistic of 5.23 indicates that there is some difference between the group means, but we need to assess whether this difference is statistically significant.\n",
    "\n",
    "2. **P-Value (0.02):**\n",
    "   - The p-value is the probability of observing the data if the null hypothesis (no significant differences between group means) is true.\n",
    "   - A low p-value suggests that the observed results are unlikely to have occurred by random chance if the null hypothesis is true.\n",
    "   - In your case, a p-value of 0.02 indicates that the observed differences between the group means are statistically significant at a significance level of 0.05 (or 5%).\n",
    "\n",
    "**Conclusion:**\n",
    "Based on the results of the one-way ANOVA:\n",
    "- The p-value of 0.02 is less than the typical significance level of 0.05.\n",
    "- Therefore, you would reject the null hypothesis.\n",
    "- This implies that there are statistically significant differences between at least some of the group means.\n",
    "\n",
    "However, the one-way ANOVA does not tell you which specific group means are different from each other. To determine which groups differ significantly, you might need to perform post hoc tests (such as Tukey's Honestly Significant Difference test) or conduct additional analyses.\n",
    "\n",
    "Remember that statistical significance does not necessarily imply practical significance or a meaningful effect size. It's important to consider the context of the data and the size of the observed differences when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcf2d61",
   "metadata": {},
   "source": [
    "### Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea470d",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is a crucial aspect of data analysis. Missing data can arise for various reasons, such as participants dropping out of the study or incomplete responses. The way you handle missing data can impact the validity and reliability of your results. Here are some common methods for handling missing data in a repeated measures ANOVA and their potential consequences:\n",
    "\n",
    "1. **Complete Case Analysis (Listwise Deletion):**\n",
    "   - This method involves removing cases (participants) with missing data from the analysis.\n",
    "   - Consequences:\n",
    "     - Reduces the sample size, potentially leading to reduced statistical power and generalizability.\n",
    "     - May introduce bias if the missing data are not missing completely at random (MCAR).\n",
    "\n",
    "2. **Mean Imputation:**\n",
    "   - Missing values are replaced with the mean of the observed values for that variable.\n",
    "   - Consequences:\n",
    "     - Underestimates the standard errors and can lead to biased parameter estimates.\n",
    "     - Can reduce the variability in the data and potentially affect the statistical significance of results.\n",
    "\n",
    "3. **Last Observation Carried Forward (LOCF):**\n",
    "   - Missing values are replaced with the last observed value for that participant.\n",
    "   - Consequences:\n",
    "     - Assumes that the participant's last observed value is a valid representation of the missing data, which may not be true.\n",
    "\n",
    "4. **Linear Interpolation:**\n",
    "   - Missing values are estimated based on the linear trend between adjacent observed values.\n",
    "   - Consequences:\n",
    "     - Can be more accurate than simpler methods but may introduce additional variability.\n",
    "\n",
    "5. **Multiple Imputation:**\n",
    "   - Multiple datasets are created with imputed values, and the analysis is performed on each dataset. The results are then combined to obtain more accurate estimates.\n",
    "   - Consequences:\n",
    "     - Requires additional computational resources and assumptions about the distribution of missing data.\n",
    "     - Provides more valid estimates compared to simpler methods if assumptions are met.\n",
    "\n",
    "6. **Mixed-Effects Models (Longitudinal Data Analysis):**\n",
    "   - A more advanced approach that utilizes all available data while accounting for the correlation between repeated measures within the same participant.\n",
    "   - Consequences:\n",
    "     - Requires a more complex model and may be more challenging to implement.\n",
    "\n",
    "Choosing the appropriate method for handling missing data depends on the nature of the data, the reason for missingness, and the assumptions that can reasonably be made. It's important to consider the potential biases and limitations introduced by each method. Multiple imputation and mixed-effects models are generally considered more robust approaches for handling missing data in repeated measures ANOVA, as they can provide more valid and reliable results while accounting for the correlation structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705c49b",
   "metadata": {},
   "source": [
    "### Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6a922",
   "metadata": {},
   "source": [
    "Post-hoc tests are used after conducting an Analysis of Variance (ANOVA) to make pairwise comparisons between group means when a significant difference is found. ANOVA tells us whether there are overall differences among groups, but it doesn't specify which specific groups are different from each other. Post-hoc tests help address this issue by providing more detailed information about group differences. Here are some common post-hoc tests and when to use them:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD) Test:**\n",
    "   - Use: When you have more than two groups and you want to compare all possible pairs of means.\n",
    "   - Example: A researcher conducts an experiment comparing the effectiveness of three different teaching methods on student test scores. After performing ANOVA and finding a significant difference, Tukey's HSD test can be used to determine which specific teaching methods lead to significantly different outcomes.\n",
    "\n",
    "2. **Bonferroni Correction:**\n",
    "   - Use: When you are making multiple pairwise comparisons and want to control for the overall Type I error rate (i.e., the risk of making a false positive).\n",
    "   - Example: An experiment involves testing the effects of a new drug on multiple outcome variables. To avoid inflating the overall Type I error rate, the researcher uses the Bonferroni correction when conducting post-hoc tests on the different outcome variables.\n",
    "\n",
    "3. **Dunn's Test (Nonparametric):**\n",
    "   - Use: When the assumptions of normality and homoscedasticity are violated or when dealing with ordinal data. It is similar to Tukey's HSD but suitable for non-normal data.\n",
    "   - Example: A study compares the completion times of three different types of tasks using participants with varying skill levels. If the data are not normally distributed, Dunn's test can be used as a nonparametric alternative to Tukey's HSD.\n",
    "\n",
    "4. **Scheffe's Test:**\n",
    "   - Use: When you have a small sample size and want to control the overall Type I error rate while making pairwise comparisons.\n",
    "   - Example: An experiment investigates the effects of three different diets on weight loss in a small group of participants. Scheffe's test can be used to make post-hoc comparisons while controlling for the overall Type I error rate.\n",
    "\n",
    "5. **Games-Howell Test:**\n",
    "   - Use: When the assumption of homogeneity of variances is violated (heteroscedasticity) and sample sizes are unequal.\n",
    "   - Example: An analysis compares the performance of three different exercise regimens on a health parameter, but the groups have different sample sizes and unequal variances. Games-Howell test can be used to compare group means.\n",
    "\n",
    "When to Use Post-hoc Tests:\n",
    "Post-hoc tests are necessary when the overall ANOVA indicates a significant difference among group means. They provide specific information about which groups are different from each other, helping to identify the specific sources of difference and avoid making incorrect or premature conclusions about group comparisons. Post-hoc tests are especially important when dealing with multiple group comparisons to avoid potential Type I errors and ensure accurate interpretations of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7a23b",
   "metadata": {},
   "source": [
    "### Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "### 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "### to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6b09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 669.9406249090015\n",
      "P-value: 1.366209078537346e-74\n",
      "Reject the null hypothesis: There are significant differences between the mean weight loss of the diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Simulated weight loss data for each diet\n",
    "diet_A = np.array([2.5, 3.1, 2.8, 3.0, 2.9, 3.2, 3.5, 3.3, 2.7, 3.0,\n",
    "                   2.8, 3.1, 3.4, 3.2, 2.9, 3.0, 2.7, 3.3, 3.1, 3.5,\n",
    "                   3.2, 3.0, 3.4, 2.9, 3.1, 3.2, 3.0, 2.8, 2.9, 3.3,\n",
    "                   2.7, 3.2, 3.4, 3.1, 2.8, 3.0, 2.9, 3.2, 3.5, 3.3,\n",
    "                   2.7, 3.0, 2.8, 3.1, 3.4, 3.2, 2.9, 3.0, 2.7, 3.3])\n",
    "\n",
    "diet_B = np.array([1.8, 2.0, 1.9, 2.1, 1.7, 2.2, 2.3, 2.5, 2.4, 2.3,\n",
    "                   2.1, 2.0, 1.9, 2.2, 2.1, 1.8, 2.0, 1.9, 2.1, 1.7,\n",
    "                   2.2, 2.3, 2.5, 2.4, 2.3, 2.1, 2.0, 1.9, 2.2, 2.1,\n",
    "                   1.8, 2.0, 1.9, 2.1, 1.7, 2.2, 2.3, 2.5, 2.4, 2.3,\n",
    "                   2.1, 2.0, 1.9, 2.2, 2.1, 1.8, 2.0, 1.9, 2.1, 1.7])\n",
    "\n",
    "diet_C = np.array([1.2, 1.5, 1.4, 1.6, 1.3, 1.7, 1.8, 1.6, 1.5, 1.4,\n",
    "                   1.2, 1.5, 1.3, 1.7, 1.6, 1.4, 1.5, 1.3, 1.7, 1.8,\n",
    "                   1.6, 1.5, 1.4, 1.2, 1.5, 1.3, 1.7, 1.6, 1.4, 1.5,\n",
    "                   1.2, 1.5, 1.4, 1.6, 1.3, 1.7, 1.8, 1.6, 1.5, 1.4,\n",
    "                   1.2, 1.5, 1.3, 1.7, 1.6, 1.4, 1.5, 1.3, 1.7, 1.8])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There are significant differences between the mean weight loss of the diets.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There are no significant differences between the mean weight loss of the diets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a5e7b",
   "metadata": {},
   "source": [
    "### Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "### complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "### randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "### complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "### interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0a3b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Main Effect - F-statistic: 0.193669715308595\n",
      "Software Main Effect - P-value: 0.8242968493119679\n",
      "Experience Main Effect - F-statistic: 1.4797363813278055\n",
      "Experience Main Effect - P-value: 0.22722286070941386\n",
      "Interaction Effect - F-statistic: 1.219018137012568\n",
      "Interaction Effect - P-value: 0.3006938566718389\n",
      "Fail to reject the null hypothesis: There is no significant main effect of software programs.\n",
      "Fail to reject the null hypothesis: There is no significant main effect of employee experience.\n",
      "Fail to reject the null hypothesis: There is no significant interaction effect.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data for software programs (A, B, C)\n",
    "software = np.random.choice(['A', 'B', 'C'], size=90)\n",
    "\n",
    "# Generate data for employee experience (novice, experienced)\n",
    "experience = np.random.choice(['Novice', 'Experienced'], size=90)\n",
    "\n",
    "# Generate task completion times\n",
    "task_times = np.random.normal(loc=15, scale=3, size=90)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Software': software, 'Experience': experience, 'Time': task_times})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols('Time ~ C(Software) + C(Experience) + C(Software):C(Experience)', data=data).fit()\n",
    "\n",
    "# Perform the ANOVA\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract F-statistics and p-values\n",
    "f_statistic_software = anova_table.loc['C(Software)', 'F']\n",
    "p_value_software = anova_table.loc['C(Software)', 'PR(>F)']\n",
    "\n",
    "f_statistic_experience = anova_table.loc['C(Experience)', 'F']\n",
    "p_value_experience = anova_table.loc['C(Experience)', 'PR(>F)']\n",
    "\n",
    "interaction_f_statistic = anova_table.loc['C(Software):C(Experience)', 'F']\n",
    "interaction_p_value = anova_table.loc['C(Software):C(Experience)', 'PR(>F)']\n",
    "\n",
    "print(\"Software Main Effect - F-statistic:\", f_statistic_software)\n",
    "print(\"Software Main Effect - P-value:\", p_value_software)\n",
    "\n",
    "print(\"Experience Main Effect - F-statistic:\", f_statistic_experience)\n",
    "print(\"Experience Main Effect - P-value:\", p_value_experience)\n",
    "\n",
    "print(\"Interaction Effect - F-statistic:\", interaction_f_statistic)\n",
    "print(\"Interaction Effect - P-value:\", interaction_p_value)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "if p_value_software < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant main effect of software programs.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant main effect of software programs.\")\n",
    "\n",
    "if p_value_experience < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant main effect of employee experience.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant main effect of employee experience.\")\n",
    "\n",
    "if interaction_p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant interaction effect between software programs and employee experience.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant interaction effect.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d848657",
   "metadata": {},
   "source": [
    "### Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "### scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "### experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "### two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "### between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58133e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-sample t-test:\n",
      "T-statistic: -4.754695943505281\n",
      "P-value: 3.819135262679478e-06\n",
      "Reject the null hypothesis: There is a significant difference in test scores between the two groups.\n",
      "\n",
      "Post-hoc Tukey's HSD test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   6.2615   0.0 3.6645 8.8585   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated test score data for control and experimental groups\n",
    "np.random.seed(42)\n",
    "control_group = np.random.normal(loc=70, scale=10, size=100)\n",
    "experimental_group = np.random.normal(loc=75, scale=10, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"Two-sample t-test:\")\n",
    "print(\"T-statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in test scores between the two groups.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in test scores between the two groups.\")\n",
    "\n",
    "# Perform post-hoc test (Tukey's HSD) if the results are significant\n",
    "if p_value < alpha:\n",
    "    data = pd.DataFrame({'Group': np.repeat(['Control', 'Experimental'], 100),\n",
    "                         'Scores': np.concatenate([control_group, experimental_group])})\n",
    "\n",
    "    tukey_results = pairwise_tukeyhsd(data['Scores'], data['Group'], alpha=0.05)\n",
    "    print(\"\\nPost-hoc Tukey's HSD test:\")\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426074c",
   "metadata": {},
   "source": [
    "### Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "### retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "### on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a posthoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62203955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-way ANOVA:\n",
      "F-statistic: 3.4668585311085742\n",
      "P-value: 0.03559195817852438\n",
      "Reject the null hypothesis: There is a significant difference in daily sales between the three stores.\n",
      "\n",
      "Post-hoc Tukey's HSD test:\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "======================================================\n",
      "group1 group2 meandiff p-adj   lower    upper   reject\n",
      "------------------------------------------------------\n",
      "     A      B 115.8201 0.037    5.6782 225.9621   True\n",
      "     A      C  90.0775 0.131  -20.0645 200.2194  False\n",
      "     B      C -25.7426 0.843 -135.8846  84.3993  False\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated sales data for Store A, Store B, and Store C\n",
    "np.random.seed(42)\n",
    "store_A_sales = np.random.normal(loc=1000, scale=200, size=30)\n",
    "store_B_sales = np.random.normal(loc=1100, scale=180, size=30)\n",
    "store_C_sales = np.random.normal(loc=1050, scale=190, size=30)\n",
    "\n",
    "# Combine the data\n",
    "all_sales = np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n",
    "store_labels = np.repeat(['A', 'B', 'C'], 30)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "\n",
    "print(\"One-way ANOVA:\")\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference in daily sales between the three stores.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference in daily sales between the three stores.\")\n",
    "\n",
    "# Perform post-hoc test (Tukey's HSD) if the results are significant\n",
    "if p_value < alpha:\n",
    "    data = pd.DataFrame({'Store': store_labels, 'Sales': all_sales})\n",
    "\n",
    "    tukey_results = pairwise_tukeyhsd(data['Sales'], data['Store'], alpha=0.05)\n",
    "    print(\"\\nPost-hoc Tukey's HSD test:\")\n",
    "    print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b812d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
