{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce35ea6",
   "metadata": {},
   "source": [
    "## Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c50ca0",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) in its primal form is:\n",
    "\n",
    "minimize (1/2) ||w||^2\n",
    "subject to y_i (w^T x_i + b) >= 1, for all i\n",
    "\n",
    "Here:\n",
    "- w is the weight vector, which defines the direction of the hyperplane.\n",
    "- x_i is the i-th training sample.\n",
    "- y_i is the label of the i-th training sample, which can be either -1 or 1.\n",
    "- b is the bias term, which shifts the hyperplane away from the origin.\n",
    "- ||w||^2 is the squared Euclidean norm of w, which is used for regularization to prevent overfitting.\n",
    "\n",
    "The goal of the linear SVM is to find the values of w and b that minimize the objective function while satisfying the constraints. The constraints ensure that the hyperplane correctly classifies all the training samples, and that it is as far away as possible from the nearest samples, which are known as the support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406adfa",
   "metadata": {},
   "source": [
    "## Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77705f26",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) in its primal form is:\n",
    "\n",
    "minimize (1/2) ||w||^2\n",
    "\n",
    "Here, w is the weight vector, which defines the direction of the hyperplane, and ||w||^2 is the squared Euclidean norm of w, which is used for regularization to prevent overfitting.\n",
    "\n",
    "The goal of the linear SVM is to find the values of w and b (the bias term) that minimize the objective function while satisfying the constraints:\n",
    "\n",
    "y\\_i (w^T x\\_i + b) >= 1, for all i\n",
    "\n",
    "Here, x\\_i is the i-th training sample, y\\_i is the label of the i-th training sample, which can be either -1 or 1. The constraints ensure that the hyperplane correctly classifies all the training samples and that it is as far away as possible from the nearest samples, which are known as the support vectors.\n",
    "\n",
    "In summary, the objective function of a linear SVM is to minimize the squared Euclidean norm of the weight vector, subject to the constraints that the hyperplane correctly classifies all the training samples and is as far away as possible from the nearest samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a129431",
   "metadata": {},
   "source": [
    "## Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005617e0",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in Support Vector Machines (SVMs) and other kernel-based methods to transform the original input data into a higher-dimensional feature space, where the data can be more easily separated or modeled.\n",
    "\n",
    "In SVMs, the kernel trick is used to enable the construction of non-linear decision boundaries, while still maintaining the computational efficiency and mathematical elegance of the linear SVM formulation. This is achieved by replacing the dot product between the input vectors in the linear SVM formulation with a non-linear kernel function, which implicitly maps the input vectors to a higher-dimensional feature space.\n",
    "\n",
    "The most commonly used kernel functions are the polynomial kernel, the radial basis function (RBF) kernel, and the sigmoid kernel. The choice of kernel function depends on the nature of the data and the problem being solved.\n",
    "\n",
    "The key advantage of the kernel trick is that it allows us to work with the high-dimensional feature space implicitly, without having to explicitly compute the coordinates of the data in the feature space. This is because the kernel function only depends on the dot product between the input vectors, which can be computed directly in the input space.\n",
    "\n",
    "In summary, the kernel trick is a technique used in SVMs and other kernel-based methods to transform the input data into a higher-dimensional feature space, where the data can be more easily separated or modeled. This is achieved by using a non-linear kernel function to replace the dot product between the input vectors in the linear SVM formulation, enabling the construction of non-linear decision boundaries while still maintaining computational efficiency and mathematical elegance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a35c9",
   "metadata": {},
   "source": [
    "## What is the role of support vectors in SVM Explain with example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2bd541",
   "metadata": {},
   "source": [
    "Support vectors are a crucial component of Support Vector Machines (SVMs), as they play a key role in defining the optimal decision boundary that separates the classes in the data.\n",
    "\n",
    "In SVMs, the goal is to find the hyperplane that maximally separates the classes, while also minimizing the classification error. The hyperplane is defined by the weight vector w and the bias term b, and the distance between the hyperplane and the nearest data points is called the margin. The optimal hyperplane is the one that maximizes the margin.\n",
    "\n",
    "The data points that lie closest to the hyperplane are called support vectors. These points are important because they are used to define the margin and to constrain the optimization problem that is used to find the optimal hyperplane. Specifically, the optimization problem is subject to the constraint that the distance between the hyperplane and the support vectors is equal to the margin.\n",
    "\n",
    "To illustrate the role of support vectors in SVMs, consider a simple example with two classes of data that are linearly separable. The data points are shown in the following figure, along with the optimal hyperplane that separates the classes:\n",
    "\n",
    "In this example, there are three support vectors, which are indicated by the circles. The two support vectors on the left side of the hyperplane are the closest data points to the hyperplane in the negative class, and the one support vector on the right side of the hyperplane is the closest data point to the hyperplane in the positive class.\n",
    "\n",
    "The margin is defined as the distance between the hyperplane and the support vectors, and is indicated by the dashed lines in the figure. The optimization problem that is used to find the optimal hyperplane is subject to the constraint that the distance between the hyperplane and the support vectors is equal to the margin.\n",
    "\n",
    "In summary, support vectors are the data points that lie closest to the hyperplane in SVMs, and are used to define the margin and to constrain the optimization problem that is used to find the optimal hyperplane. The optimal hyperplane is the one that maximizes the margin, and is determined by the weight vector w and the bias term b. The role of support vectors is to ensure that the hyperplane is the optimal one that separates the classes in the data with the maximum possible margin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705cad0",
   "metadata": {},
   "source": [
    "## Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf19844",
   "metadata": {},
   "source": [
    "Sure, I can provide examples and graphs to illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVMs.\n",
    "\n",
    "1. Hyperplane:\n",
    "\n",
    "A hyperplane is a linear decision boundary that separates the classes in the data. In SVMs, the goal is to find the hyperplane that maximally separates the classes, while also minimizing the classification error.\n",
    "\n",
    "Here's an example of a hyperplane in a two-dimensional feature space:\n",
    "\n",
    "In this example, the hyperplane is represented by the solid line, and it separates the positive class (red circles) from the negative class (blue squares).\n",
    "\n",
    "2. Marginal plane:\n",
    "\n",
    "The marginal plane is a pair of parallel hyperplanes that are equidistant from the optimal hyperplane. The distance between the marginal plane and the optimal hyperplane is called the margin.\n",
    "\n",
    "Here's an example of a marginal plane in a two-dimensional feature space:\n",
    "\n",
    "In this example, the optimal hyperplane is represented by the solid line, and the marginal plane is represented by the dashed lines. The distance between the marginal plane and the optimal hyperplane is the margin, which is indicated by the arrows.\n",
    "\n",
    "3. Soft margin:\n",
    "\n",
    "In practice, the data may not be perfectly separable by a hyperplane. In such cases, SVMs can be modified to allow for some misclassifications, while still maximizing the margin. This is known as a soft margin SVM.\n",
    "\n",
    "Here's an example of a soft margin SVM in a two-dimensional feature space:\n",
    "\n",
    "In this example, the data is not perfectly separable by a hyperplane. The soft margin SVM allows for some misclassifications by introducing slack variables, which are represented by the vertical lines. The optimal hyperplane is represented by the solid line, and it maximizes the margin while also minimizing the classification error.\n",
    "\n",
    "4. Hard margin:\n",
    "\n",
    "A hard margin SVM is a special case of a soft margin SVM, where the slack variables are all set to zero. In other words, a hard margin SVM assumes that the data is perfectly separable by a hyperplane.\n",
    "\n",
    "Here's an example of a hard margin SVM in a two-dimensional feature space:\n",
    "\n",
    "In this example, the data is perfectly separable by a hyperplane. The hard margin SVM finds the optimal hyperplane that maximally separates the classes, while also ensuring that there are no misclassifications.\n",
    "\n",
    "In summary, the hyperplane is a linear decision boundary that separates the classes in the data. The marginal plane is a pair of parallel hyperplanes that are equidistant from the optimal hyperplane, and the distance between them is the margin. Soft margin SVMs allow for some misclassifications, while still maximizing the margin, and hard margin SVMs assume that the data is perfectly separable by a hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493514b",
   "metadata": {},
   "source": [
    "## Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e1d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbe681b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_diabetes()\n",
    "x=data['data']\n",
    "y=data['target']\n",
    "feature_names=data['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc329ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "         0.01990749, -0.01764613],\n",
       "       [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "        -0.06833155, -0.09220405],\n",
       "       [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "         0.00286131, -0.02593034],\n",
       "       ...,\n",
       "       [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "        -0.04688253,  0.01549073],\n",
       "       [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "         0.04452873, -0.02593034],\n",
       "       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "        -0.00422151,  0.00306441]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a179b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47d6e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44c977a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018114</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081413</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019907 -0.017646  \n",
       "1   -0.039493 -0.068332 -0.092204  \n",
       "2   -0.002592  0.002861 -0.025930  \n",
       "3    0.034309  0.022688 -0.009362  \n",
       "4   -0.002592 -0.031988 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018114  0.044485  \n",
       "439 -0.011080 -0.046883  0.015491  \n",
       "440  0.026560  0.044529 -0.025930  \n",
       "441 -0.039493 -0.004222  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "x=pd.DataFrame(x,columns=feature_names)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dab85339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome\n",
       "0      151.0\n",
       "1       75.0\n",
       "2      141.0\n",
       "3      206.0\n",
       "4      135.0\n",
       "..       ...\n",
       "437    178.0\n",
       "438    104.0\n",
       "439    132.0\n",
       "440    220.0\n",
       "441     57.0\n",
       "\n",
       "[442 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=pd.DataFrame(y,columns=['outcome'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea9c91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82fe127e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((353, 10), (89, 10))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2704a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumanth\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr=SVR()\n",
    "svr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdf88c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bf5ae4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eef7048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        , -1.        , -1.        , -1.        ,  1.        ,\n",
       "        -1.        , -0.16948097, -1.        ,  1.        , -0.70910852,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n",
       "        -1.        ,  1.        , -1.        ,  1.        , -1.        ,\n",
       "         1.        , -1.        , -1.        ,  1.        , -1.        ,\n",
       "         1.        ,  1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        , -1.        , -1.        , -1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        , -1.        ,  1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        ,  1.        , -1.        ,\n",
       "         1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        ,  1.        , -1.        ,  1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        ,  1.        , -0.39198651,  1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        , -1.        , -1.        ,  1.        ,  1.        ,\n",
       "         1.        , -1.        ,  1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n",
       "        -1.        ,  1.        ,  1.        , -1.        ,  1.        ,\n",
       "         1.        ,  1.        , -1.        , -1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        , -0.85623459, -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n",
       "         1.        , -1.        , -1.        , -1.        ,  1.        ,\n",
       "         1.        ,  1.        , -1.        , -1.        ,  1.        ,\n",
       "         1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "        -1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        ,  1.        ,  1.        , -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        , -1.        ,  1.        ,\n",
       "        -1.        , -1.        , -1.        ,  1.        , -1.        ,\n",
       "         1.        ,  1.        ,  1.        ,  1.        , -1.        ,\n",
       "        -1.        , -1.        , -1.        , -1.        ,  1.        ,\n",
       "        -1.        ,  1.        , -1.        , -1.        , -1.        ,\n",
       "         1.        , -1.        ,  1.        , -1.        , -1.        ,\n",
       "        -0.8731894 ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
       "        -1.        ,  1.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54334cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is 18.211365770500286\n",
      "mean squared error is 4333.285954518086\n",
      "mean absolute error is  56.02372412801096\n"
     ]
    }
   ],
   "source": [
    "y_pred=svr.predict(x_test)\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "r2=r2_score(y_test,y_pred)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "print('r2 score is',r2*100)\n",
    "print('mean squared error is',mse)\n",
    "print('mean absolute error is ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96b90e1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     outcome\n",
       "287    219.0\n",
       "211     70.0\n",
       "72     202.0\n",
       "321    230.0\n",
       "73     111.0\n",
       "..       ...\n",
       "255    153.0\n",
       "90      98.0\n",
       "57      37.0\n",
       "391     63.0\n",
       "24     184.0\n",
       "\n",
       "[89 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e4cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumanth\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Sumanth\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NuSVR()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NuSVR</label><div class=\"sk-toggleable__content\"><pre>NuSVR()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NuSVR()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR,NuSVR\n",
    "svr1=LinearSVR()\n",
    "svr2=NuSVR()\n",
    "svr1.fit(x_train,y_train)\n",
    "svr2.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48f0f45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score for linearis -27.232785634821767\n",
      "mean squared error for linear is 6740.986056797689\n",
      "mean absolute error for linear svr is  63.284693419483766\n"
     ]
    }
   ],
   "source": [
    "y_pred=svr1.predict(x_test)\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "r2=r2_score(y_test,y_pred)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "print('r2 score for linearis',r2*100)\n",
    "print('mean squared error for linear is',mse)\n",
    "print('mean absolute error for linear svr is ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a58274a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score for nusvr is 16.831999301920586\n",
      "mean squared error for nusvr is 4406.366883191594\n",
      "mean absolute error for nusvr is  57.650730289526834\n"
     ]
    }
   ],
   "source": [
    "y_pred=svr2.predict(x_test)\n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n",
    "r2=r2_score(y_test,y_pred)\n",
    "mae=mean_absolute_error(y_test,y_pred)\n",
    "mse=mean_squared_error(y_test,y_pred)\n",
    "print('r2 score for nusvr is',r2*100)\n",
    "print('mean squared error for nusvr is',mse)\n",
    "print('mean absolute error for nusvr is ',mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba79ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
