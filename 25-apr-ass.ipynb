{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd142e69",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66a805",
   "metadata": {},
   "source": [
    "Eigenvalues and Eigenvectors are important concepts in linear algebra and are widely used in various fields such as computer science, physics, and engineering.\n",
    "\n",
    "An Eigenvector is a non-zero vector that, when multiplied by a square matrix, produces a scalar multiple of itself. The scalar multiple is called the Eigenvalue corresponding to the Eigenvector. In other words, if A is a square matrix, v is a non-zero vector, and λ is a scalar, then v is an Eigenvector of A and λ is the corresponding Eigenvalue if and only if Av = λv.\n",
    "\n",
    "Eigen-Decomposition, also known as Eigendecomposition or Spectral Decomposition, is a matrix decomposition technique that involves expressing a square matrix as a product of three matrices: a matrix of Eigenvectors, a diagonal matrix of Eigenvalues, and the inverse of the Eigenvector matrix. The Eigen-Decomposition of a matrix A can be represented as:\n",
    "\n",
    "A = QΛQ^T\n",
    "\n",
    "where Q is a matrix of Eigenvectors, Λ is a diagonal matrix of Eigenvalues, and Q^T is the transpose of Q.\n",
    "\n",
    "The Eigen-Decomposition approach is widely used in various applications such as image compression, facial recognition, and data visualization. For example, let's consider a dataset of 2D points that are scattered around a line. We can use PCA (Principal Component Analysis), which is based on the Eigen-Decomposition approach, to identify the direction of the line and reduce the dimensionality of the data.\n",
    "\n",
    "The first step in PCA is to compute the covariance matrix of the data. The covariance matrix is a square matrix that measures the pairwise covariance between the features of the dataset. For a 2D dataset, the covariance matrix is a 2x2 matrix.\n",
    "\n",
    "The next step is to compute the Eigenvectors and Eigenvalues of the covariance matrix. The Eigenvectors represent the directions of the maximum variance in the data, and the corresponding Eigenvalues represent the amount of variance in the data along each direction.\n",
    "\n",
    "In our example, the first principal component corresponds to the direction of the line, and the second principal component corresponds to the direction perpendicular to the line. The first principal component has a higher variance than the second principal component, and therefore, it captures the most important information in the data.\n",
    "\n",
    "By retaining only the first principal component, we can effectively reduce the dimensionality of the data and eliminate the noise and redundancy in the less important direction. The resulting transformed data has a lower dimensionality and is easier to visualize and analyze.\n",
    "\n",
    "In summary, Eigenvalues and Eigenvectors are important concepts in linear algebra and are used in the Eigen-Decomposition approach to decompose a square matrix into three matrices: a matrix of Eigenvectors, a diagonal matrix of Eigenvalues, and the inverse of the Eigenvector matrix. The Eigen-Decomposition approach is widely used in various applications such as PCA, image compression, and facial recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b693066",
   "metadata": {},
   "source": [
    "## Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad2f454",
   "metadata": {},
   "source": [
    "Eigen decomposition, also known as eigenvalue decomposition or spectral decomposition, is a technique in linear algebra used to decompose a square matrix into a set of eigenvectors and eigenvalues. The significance of eigen decomposition lies in its ability to simplify complex mathematical operations, such as matrix multiplication and inversion, and to reveal important properties of a matrix, such as its rank, determinant, and trace.\n",
    "\n",
    "In eigen decomposition, a square matrix A is decomposed into the following form:\n",
    "\n",
    "A = QΛQ^T\n",
    "\n",
    "where Q is an orthogonal matrix whose columns are the eigenvectors of A, Λ is a diagonal matrix whose entries are the corresponding eigenvalues, and Q^T is the transpose of Q.\n",
    "\n",
    "The significance of eigen decomposition can be seen in the following ways:\n",
    "\n",
    "1. Simplification of matrix operations: Eigen decomposition can simplify matrix operations, such as matrix multiplication and inversion. For example, if we want to compute the power of a matrix A, i.e., A^n, we can first decompose A into QΛQ^T, and then compute the power as (QΛQ^T)^n = QΛ^nQ^T. This is much more efficient than directly computing A^n, especially for large matrices.\n",
    "2. Revealing matrix properties: Eigen decomposition can reveal important properties of a matrix, such as its rank, determinant, and trace. For example, the rank of a matrix is equal to the number of non-zero eigenvalues, the determinant is equal to the product of the eigenvalues, and the trace is equal to the sum of the eigenvalues.\n",
    "3. Applications in data analysis: Eigen decomposition is widely used in data analysis, such as in principal component analysis (PCA) and factor analysis. In PCA, eigen decomposition is used to decompose the covariance matrix of a dataset into a set of eigenvectors and eigenvalues, which can then be used to identify the most important features or dimensions of the data.\n",
    "\n",
    "In summary, eigen decomposition is a powerful technique in linear algebra used to decompose a square matrix into a set of eigenvectors and eigenvalues. Its significance lies in its ability to simplify complex mathematical operations, reveal important properties of a matrix, and its applications in data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1cfbc",
   "metadata": {},
   "source": [
    "## Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb3a5d",
   "metadata": {},
   "source": [
    "A square matrix A is diagonalizable using the Eigen-Decomposition approach if and only if it satisfies the following two conditions:\n",
    "\n",
    "1. A has n linearly independent eigenvectors, where n is the size of the matrix.\n",
    "2. The algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity.\n",
    "\n",
    "Here's a brief proof to support these conditions:\n",
    "\n",
    "First, let's assume that A has n linearly independent eigenvectors, and let's denote them as v1, v2, ..., vn. We can then form a matrix Q whose columns are these eigenvectors, i.e., Q = [v1, v2, ..., vn]. Since the eigenvectors are linearly independent, Q is invertible.\n",
    "\n",
    "Now, let's denote the eigenvalues corresponding to these eigenvectors as λ1, λ2, ..., λn, and let's form a diagonal matrix Λ whose entries are these eigenvalues, i.e., Λ = diag(λ1, λ2, ..., λn).\n",
    "\n",
    "By definition, we have Av1 = λ1v1, Av2 = λ2v2, ..., Avn = λnwn. We can then write these equations in matrix form as:\n",
    "\n",
    "AQ = QΛ\n",
    "\n",
    "Multiplying both sides by Q^T, we get:\n",
    "\n",
    "Q^TAQ = Λ\n",
    "\n",
    "Since Q is invertible, we can multiply both sides by Q to get:\n",
    "\n",
    "A = QΛQ^T\n",
    "\n",
    "which is the desired Eigen-Decomposition of A.\n",
    "\n",
    "Now, let's assume that A satisfies the second condition, i.e., the algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity. This means that for each eigenvalue λi, there are exactly mi linearly independent eigenvectors, where mi is the multiplicity of λi.\n",
    "\n",
    "We can then form a matrix Q whose columns are the linearly independent eigenvectors for each eigenvalue, i.e., Q = [v11, v12, ..., v1m1, v21, v22, ..., v2m2, ..., vnm1, vnm2, ..., vnmm].\n",
    "\n",
    "Using a similar argument as before, we can show that Q is invertible, and that AQ = QΛ, where Λ is a block diagonal matrix whose entries are the eigenvalues λi repeated mi times.\n",
    "\n",
    "Multiplying both sides by Q^T, we get:\n",
    "\n",
    "Q^TAQ = Λ\n",
    "\n",
    "which is the desired Eigen-Decomposition of A.\n",
    "\n",
    "In summary, a square matrix A is diagonalizable using the Eigen-Decomposition approach if and only if it has n linearly independent eigenvectors and the algebraic multiplicity of each eigenvalue is equal to its geometric multiplicity. These conditions ensure that we can form an invertible matrix Q whose columns are the eigenvectors of A, and a diagonal matrix Λ whose entries are the eigenvalues of A, such that A = QΛQ^T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7ec02",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406faaf7",
   "metadata": {},
   "source": [
    "The spectral theorem is a fundamental result in linear algebra that states that any symmetric, real-valued matrix can be decomposed into a set of orthogonal eigenvectors and corresponding eigenvalues. Specifically, if A is a symmetric, real-valued matrix, then there exists an orthogonal matrix Q (i.e., Q^TQ = I) and a diagonal matrix Λ such that:\n",
    "\n",
    "A = QΛQ^T\n",
    "\n",
    "The significance of the spectral theorem in the context of the Eigen-Decomposition approach is that it guarantees the diagonalizability of any symmetric, real-valued matrix. In other words, if a matrix satisfies the conditions of the spectral theorem, then it can be decomposed into a set of eigenvectors and eigenvalues using the Eigen-Decomposition approach.\n",
    "\n",
    "To illustrate the relationship between the spectral theorem and the diagonalizability of a matrix, let's consider an example. Suppose we have the following symmetric, real-valued matrix:\n",
    "\n",
    "A = [4, 2; 2, 3]\n",
    "\n",
    "We can compute the eigenvalues and eigenvectors of A using the Eigen-Decomposition approach. Specifically, we can solve the characteristic equation det(A - λI) = 0 to find the eigenvalues, and then plug each eigenvalue back into the equation (A - λI)v = 0 to find the corresponding eigenvectors.\n",
    "\n",
    "Doing so, we find that the eigenvalues of A are λ1 = 5 and λ2 = 2, and the corresponding eigenvectors are v1 = [1/√2, 1/√2] and v2 = [-1/√2, 1/√2].\n",
    "\n",
    "We can then form the matrices Q and Λ as follows:\n",
    "\n",
    "Q = [1/√2, -1/√2; 1/√2, 1/√2]\n",
    "\n",
    "Λ = [5, 0; 0, 2]\n",
    "\n",
    "Notice that Q is an orthogonal matrix (i.e., Q^TQ = I), and Λ is a diagonal matrix, as required by the spectral theorem. Moreover, we can verify that A = QΛQ^T by multiplying out the matrices on the right-hand side.\n",
    "\n",
    "In summary, the spectral theorem is a fundamental result in linear algebra that guarantees the diagonalizability of any symmetric, real-valued matrix. The significance of the spectral theorem in the context of the Eigen-Decomposition approach is that it provides a theoretical foundation for the diagonalizability of matrices, and it ensures that the Eigen-Decomposition approach can be applied to a wide range of matrices in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3faa19",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ade98",
   "metadata": {},
   "source": [
    "To find the eigenvalues of a matrix, we need to solve the characteristic equation of the matrix. The characteristic equation is defined as:\n",
    "\n",
    "det(A - λI) = 0\n",
    "\n",
    "where A is the matrix, λ is an eigenvalue, I is the identity matrix, and det(·) is the determinant of a matrix.\n",
    "\n",
    "The eigenvalues of a matrix are the solutions to the characteristic equation. They are scalar values that represent the amount of stretching or compressing that the matrix does to a vector in the direction of the corresponding eigenvector. In other words, if v is an eigenvector of A with eigenvalue λ, then:\n",
    "\n",
    "Av = λv\n",
    "\n",
    "This means that when we multiply the vector v by the matrix A, the result is the same as if we had stretched or compressed v by a factor of λ in the direction of v.\n",
    "\n",
    "Eigenvalues can be real or complex numbers, and they can be positive, negative, or zero. The number of linearly independent eigenvectors associated with an eigenvalue is called the geometric multiplicity of the eigenvalue, and the number of times an eigenvalue appears as a root of the characteristic equation is called the algebraic multiplicity of the eigenvalue.\n",
    "\n",
    "Eigenvalues and eigenvectors have many applications in linear algebra, differential equations, physics, engineering, and computer science. For example, in principal component analysis (PCA), a widely used technique for dimensionality reduction, we use the eigenvalues and eigenvectors of the covariance matrix of a dataset to find the directions of maximum variance in the data.\n",
    "\n",
    "In summary, to find the eigenvalues of a matrix, we need to solve the characteristic equation of the matrix. The eigenvalues are scalar values that represent the amount of stretching or compressing that the matrix does to a vector in the direction of the corresponding eigenvector, and they have many applications in various fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c92cd",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85df5a7",
   "metadata": {},
   "source": [
    "Eigenvectors are non-zero vectors that, when multiplied by a square matrix, produce a scalar multiple of the original vector. The scalar multiple is called the eigenvalue corresponding to the eigenvector. In other words, if A is a square matrix, v is an eigenvector, and λ is the corresponding eigenvalue, then:\n",
    "\n",
    "Av = λv\n",
    "\n",
    "This equation is called the eigenvalue equation or the eigenvector equation.\n",
    "\n",
    "Eigenvectors and eigenvalues are closely related. The eigenvalue equation tells us that the matrix A stretches or compresses the eigenvector v by a factor of λ. In other words, the eigenvector v represents a direction in which the matrix A acts as a scalar, and the eigenvalue λ represents the magnitude of that scalar.\n",
    "\n",
    "Eigenvectors and eigenvalues have many important applications in linear algebra, differential equations, physics, engineering, and computer science. For example, in principal component analysis (PCA), a widely used technique for dimensionality reduction, we use the eigenvectors and eigenvalues of the covariance matrix of a dataset to find the directions of maximum variance in the data. In quantum mechanics, the eigenvectors and eigenvalues of the Hamiltonian operator are used to describe the energy levels and wavefunctions of a quantum system.\n",
    "\n",
    "In summary, eigenvectors are non-zero vectors that, when multiplied by a square matrix, produce a scalar multiple of the original vector. The scalar multiple is called the eigenvalue corresponding to the eigenvector. Eigenvectors and eigenvalues are closely related and have many important applications in various fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998859aa",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64439188",
   "metadata": {},
   "source": [
    "Yes, I can explain the geometric interpretation of eigenvectors and eigenvalues.\n",
    "\n",
    "Eigenvectors and eigenvalues have a geometric interpretation that can help us better understand their meaning and significance. The geometric interpretation is based on the idea that a matrix can be thought of as a linear transformation that maps vectors in a certain space to other vectors in the same space.\n",
    "\n",
    "When we multiply a vector by a matrix, the resulting vector is generally different from the original vector, both in magnitude and direction. However, there are some special vectors, called eigenvectors, that are only stretched or compressed by the matrix, but not rotated or changed in direction. In other words, when we multiply an eigenvector by a matrix, the resulting vector is a scalar multiple of the original vector, where the scalar is called the eigenvalue.\n",
    "\n",
    "The geometric interpretation of eigenvectors is that they represent the directions in which the matrix acts as a pure scaling, without any rotation or shearing. In other words, the eigenvectors are the axes of the ellipsoid or hyperboloid that the matrix transforms the unit sphere or unit hyperbola into.\n",
    "\n",
    "The geometric interpretation of eigenvalues is that they represent the amount of scaling or stretching that the matrix applies to the eigenvectors. A positive eigenvalue corresponds to a stretching in the direction of the eigenvector, while a negative eigenvalue corresponds to a compression or flipping in the direction of the eigenvector. A zero eigenvalue corresponds to a null space or a subspace that is mapped to the zero vector by the matrix.\n",
    "\n",
    "The geometric interpretation of eigen-decomposition is that it allows us to decompose a matrix into a set of orthogonal eigenvectors and corresponding eigenvalues, which can be used to represent the matrix as a combination of pure scaling transformations in the directions of the eigenvectors. This can be useful for various applications, such as image compression, principal component analysis, and solving differential equations.\n",
    "\n",
    "In summary, the geometric interpretation of eigenvectors and eigenvalues is that they represent the directions and amounts of scaling that a matrix applies to vectors in a certain space. The geometric interpretation of eigen-decomposition is that it allows us to decompose a matrix into a set of orthogonal eigenvectors and corresponding eigenvalues, which can be used to represent the matrix as a combination of pure scaling transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50bd6e",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30f2ac",
   "metadata": {},
   "source": [
    "Eigen decomposition has numerous real-world applications in various fields, including:\n",
    "\n",
    "1. Image processing and computer vision: Eigen decomposition is used in image compression, denoising, and feature extraction. For example, principal component analysis (PCA) is a widely used technique for dimensionality reduction and feature extraction in image processing, which is based on eigen decomposition.\n",
    "2. Data analysis and machine learning: Eigen decomposition is used in data analysis and machine learning for dimensionality reduction, clustering, and anomaly detection. For example, PCA is used in exploratory data analysis to visualize high-dimensional data and identify patterns and correlations.\n",
    "3. Control systems and robotics: Eigen decomposition is used in control systems and robotics for system identification, state estimation, and feedback control. For example, the linear quadratic regulator (LQR) is a control design method based on eigen decomposition, which is used in various applications such as aerospace engineering and robotics.\n",
    "4. Signal processing and communication systems: Eigen decomposition is used in signal processing and communication systems for signal filtering, modulation, and demodulation. For example, the discrete Fourier transform (DFT) and the discrete cosine transform (DCT) are based on eigen decomposition, which are widely used in signal processing and data compression.\n",
    "5. Quantum mechanics and physics: Eigen decomposition is used in quantum mechanics and physics to describe the behavior of quantum systems and physical phenomena. For example, the Schrödinger equation, which describes the time evolution of a quantum system, is based on eigen decomposition.\n",
    "6. Economics and finance: Eigen decomposition is used in economics and finance for portfolio optimization, risk management, and time series analysis. For example, the modern portfolio theory (MPT) is based on eigen decomposition, which is used to optimize the risk-return tradeoff in investment portfolios.\n",
    "\n",
    "In summary, eigen decomposition has numerous real-world applications in various fields, including image processing, data analysis, control systems, signal processing, quantum mechanics, and economics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590d284",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2211244",
   "metadata": {},
   "source": [
    "Yes, a matrix can have more than one set of eigenvectors and eigenvalues, depending on the matrix and the number of distinct eigenvalues.\n",
    "\n",
    "For an n x n matrix A, there can be up to n distinct eigenvalues, and each eigenvalue can have one or more corresponding eigenvectors. The set of all eigenvectors corresponding to a particular eigenvalue is called the eigenspace of that eigenvalue.\n",
    "\n",
    "If a matrix has n distinct eigenvalues, then it has n linearly independent eigenvectors, and the eigenspace of each eigenvalue is one-dimensional. In this case, there is only one set of eigenvectors and eigenvalues.\n",
    "\n",
    "However, if a matrix has fewer than n distinct eigenvalues, then the eigenspace of at least one eigenvalue is multi-dimensional, and there can be multiple linearly independent eigenvectors corresponding to that eigenvalue. In this case, there can be more than one set of eigenvectors and eigenvalues.\n",
    "\n",
    "For example, consider the following 2 x 2 matrix:\n",
    "\n",
    "A = [1 1; 0 1]\n",
    "\n",
    "The characteristic polynomial of A is (1-λ)^2, so there is only one distinct eigenvalue, λ = 1. The eigenspace of λ = 1 is the nullspace of (A - I), which is spanned by the vector [1; 0]. Therefore, there is only one linearly independent eigenvector, and there is only one set of eigenvectors and eigenvalues for this matrix.\n",
    "\n",
    "However, if we modify the matrix slightly to:\n",
    "\n",
    "A = [1 1; 1 0]\n",
    "\n",
    "The characteristic polynomial of A is λ^2 - λ - 1, so there are two distinct eigenvalues, λ1 = (1+√5)/2 and λ2 = (1-√5)/2. The eigenspace of each eigenvalue is one-dimensional, and there are two linearly independent eigenvectors, one for each eigenvalue. Therefore, there is only one set of eigenvectors and eigenvalues for this matrix.\n",
    "\n",
    "In summary, a matrix can have more than one set of eigenvectors and eigenvalues if it has fewer than n distinct eigenvalues and the eigenspace of at least one eigenvalue is multi-dimensional. Otherwise, there is only one set of eigenvectors and eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b618d153",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40248b45",
   "metadata": {},
   "source": [
    "Eigen-Decomposition is a powerful and versatile tool that has numerous applications in data analysis and machine learning. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "1. Principal Component Analysis (PCA): PCA is a widely used technique for dimensionality reduction and feature extraction in data analysis and machine learning. The goal of PCA is to find a lower-dimensional representation of the data that captures as much of the variance as possible. PCA is based on Eigen-Decomposition of the data covariance matrix. The eigenvectors of the covariance matrix correspond to the principal components of the data, and the eigenvalues correspond to the amount of variance explained by each principal component. By projecting the data onto the first few principal components, we can obtain a lower-dimensional representation of the data that captures most of the variance.\n",
    "2. Linear Discriminant Analysis (LDA): LDA is a supervised learning technique for dimensionality reduction and feature extraction. The goal of LDA is to find a lower-dimensional representation of the data that maximizes the separation between different classes or labels. LDA is based on Eigen-Decomposition of the within-class and between-class scatter matrices. The eigenvectors of the scatter matrices correspond to the linear discriminants of the data, and the eigenvalues correspond to the amount of separation between the classes explained by each discriminant. By projecting the data onto the first few linear discriminants, we can obtain a lower-dimensional representation of the data that maximizes the separation between the classes.\n",
    "3. Independent Component Analysis (ICA): ICA is a technique for blind source separation and feature extraction in signal processing and data analysis. The goal of ICA is to find a linear transformation of the data that separates the underlying independent sources or components. ICA is based on Eigen-Decomposition of the data covariance matrix and higher-order cumulant tensors. The eigenvectors of the covariance matrix and cumulant tensors correspond to the independent components of the data, and the eigenvalues correspond to the amount of statistical independence explained by each component. By projecting the data onto the independent components, we can obtain a representation of the data that separates the underlying sources or components.\n",
    "\n",
    "In summary, Eigen-Decomposition is a fundamental and powerful tool that has numerous applications in data analysis and machine learning, including PCA, LDA, and ICA. These techniques rely on Eigen-Decomposition to find lower-dimensional representations of the data that capture the variance, separate the classes, or separate the sources, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ef3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
